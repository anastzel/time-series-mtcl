{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zN_LW0rroNiiXCEarVWLrmF3Tvogl5Vh","timestamp":1670330113141},{"file_id":"1Id8-HYHUShoKdoE5K-pc4xUDjaVnTUFF","timestamp":1670326707174}],"mount_file_id":"1bIR7LiFOB8n3hfQ3dSpzSALbC3NhALLw","authorship_tag":"ABX9TyMB5IZ62k2Wzkjxqc4XfcLP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import sys\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from tqdm import tqdm"],"metadata":{"id":"zlOsW-IwyTvT","executionInfo":{"status":"ok","timestamp":1674819796215,"user_tz":-120,"elapsed":3406,"user":{"displayName":"Anastasios Tzelepakis","userId":"06794205193158144204"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"erRPZF0Qx8cz","executionInfo":{"status":"ok","timestamp":1674819805834,"user_tz":-120,"elapsed":2218,"user":{"displayName":"Anastasios Tzelepakis","userId":"06794205193158144204"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"78cd6c2e-0fa6-4e95-c793-37dfeb690ce0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  dataset.zip\n","  inflating: dataset/pcb_1.jpg       \n","  inflating: dataset/pcb_2.jpg       \n","  inflating: dataset/pcb_3.jpg       \n","  inflating: dataset/pcb_4.jpg       \n","  inflating: dataset/pcb_5.jpg       \n","  inflating: dataset/pcb_6.jpg       \n","   creating: dataset/typeA/\n","   creating: dataset/typeA/test/\n","  inflating: dataset/typeA/test/annotations.csv  \n","   creating: dataset/typeA/train/\n","  inflating: dataset/typeA/train/annotations.csv  \n","   creating: dataset/typeB/\n","   creating: dataset/typeB/test/\n","  inflating: dataset/typeB/test/annotations.csv  \n","   creating: dataset/typeB/train/\n","  inflating: dataset/typeB/train/annotations.csv  \n","   creating: dataset/typeC/\n","   creating: dataset/typeC/test/\n","  inflating: dataset/typeC/test/annotations.csv  \n","   creating: dataset/typeC/train/\n","  inflating: dataset/typeC/train/annotations.csv  \n","   creating: dataset/typeD/\n","   creating: dataset/typeD/test/\n","  inflating: dataset/typeD/test/annotations.csv  \n","   creating: dataset/typeD/train/\n","  inflating: dataset/typeD/train/annotations.csv  \n","   creating: dataset/typeE/\n","   creating: dataset/typeE/test/\n","  inflating: dataset/typeE/test/annotations.csv  \n","   creating: dataset/typeE/train/\n","  inflating: dataset/typeE/train/annotations.csv  \n"]}],"source":["# Copy the dataset.zip and unzip it\n","!cp \"/content/drive/MyDrive/Microchip/dataset.zip\" \"/content\"\n","!unzip dataset.zip"]},{"cell_type":"code","source":["def load_dataset(filename):\n","    # Read the .csv\n","    df = pd.read_csv(filename, delimiter=';')\n","    # Drop columns that we don't need\n","    df_cleaned = df.drop(df.columns[[0, 1, 2, 3]],axis = 1)\n","    # Set Dataframe's dtype to float32\n","    df_cleaned = df_cleaned.astype(np.float32)\n","\n","    return df_cleaned"],"metadata":{"id":"xMohYkWALxD3","executionInfo":{"status":"ok","timestamp":1674819809962,"user_tz":-120,"elapsed":4,"user":{"displayName":"Anastasios Tzelepakis","userId":"06794205193158144204"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class MicrochipDataset(Dataset):\n","    def __init__(self, pressure_ts, seq_length=8, mean=0, std=1.0):\n","        self.data = torch.tensor(pressure_ts, dtype=torch.float)        \n","        self.data = (self.data - mean) / std\n","        self.seq_length = seq_length\n","        \n","    def __len__(self):\n","        return len(self.data) - self.seq_length - 1\n","    \n","    def __getitem__(self, i):\n","        return self.data[i:(i + self.seq_length)], self.data[i + self.seq_length]"],"metadata":{"id":"ZickFVoGyXTx","executionInfo":{"status":"ok","timestamp":1674819810420,"user_tz":-120,"elapsed":6,"user":{"displayName":"Anastasios Tzelepakis","userId":"06794205193158144204"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class RegressionLSTM(nn.Module):\n","    def __init__(self, device, seq_length, hidden_units):\n","        super().__init__()\n","        self.seq_length = seq_length\n","        self.hidden_units = hidden_units\n","        self.num_layers = 1\n","        self.lstm = nn.LSTM(\n","            input_size=1,\n","            hidden_size=hidden_units,\n","            batch_first=True,\n","            num_layers=self.num_layers\n","        )\n","        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n","        self.device = device\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(2)\n","        batch_size = x.shape[0]\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, device=self.device).requires_grad_()\n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units, device=self.device).requires_grad_()\n","        _, (hn, _) = self.lstm(x, (h0, c0))\n","        out = self.linear(hn[0]).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n","        \n","        return out"],"metadata":{"id":"ISIaTXmNyXWc","executionInfo":{"status":"ok","timestamp":1674819813658,"user_tz":-120,"elapsed":528,"user":{"displayName":"Anastasios Tzelepakis","userId":"06794205193158144204"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["types = ['typeA', 'typeB', 'typeC', 'typeD', 'typeE']"],"metadata":{"id":"DJf59GlFeKSo","executionInfo":{"status":"ok","timestamp":1674819825203,"user_tz":-120,"elapsed":511,"user":{"displayName":"Anastasios Tzelepakis","userId":"06794205193158144204"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["for type in types:\n","    # Define train csv filename\n","    train_filename = f\"/content/dataset/{type}/train/annotations.csv\"\n","    test_filename = f\"/content/dataset/{type}/test/annotations.csv\"\n","\n","    print(f\"\\nPerforming time series analysis for {type}\\n\")\n","\n","    train_df = load_dataset(train_filename)\n","    test_df = load_dataset(test_filename)\n","\n","    feature_types = ['50um', '20um']\n","    # seq_lengths = list(range(1,9))\n","    seq_lengths = [1]\n","\n","    for feature in feature_types:\n","        with open(f\"lstm_logs_{feature}_{type}.txt\",'w',encoding = 'utf-8') as f:\n","            seq_losses = []\n","            for i, seq_len in enumerate(seq_lengths, start=1):\n","\n","                # Get the feature of interest\n","                train_time_series = train_df[feature]\n","                test_time_series = test_df[feature]\n","                train_ts = train_time_series.to_numpy()\n","                test_ts = test_time_series.to_numpy()\n","\n","                # Define the Sequence length\n","                seq_length = seq_len\n","\n","                # Get mean and std od training data\n","                mean = np.mean(train_ts)\n","                std = np.std(train_ts)\n","\n","                # Get the device\n","                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","                # Define the hyperparameters\n","                num_epochs = 50\n","                batch_size = 512\n","                ln_rate = 0.01\n","                num_hidden_units = 32\n","\n","                # Create train and test datasets\n","                train_dataset = MicrochipDataset(train_ts, seq_length=seq_length, mean=mean, std=std)\n","                test_dataset = MicrochipDataset(test_ts, seq_length=seq_length, mean=mean, std=std)\n","\n","                # Create dataloaders\n","                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","                test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","                test_loader_in_sample = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","                \n","                # Initialize the model ,place it to device and set it to train mode\n","                lstm_model = RegressionLSTM(device=device, seq_length=seq_length, hidden_units=num_hidden_units)\n","                lstm_model.to(device)\n","                lstm_model.train()\n","\n","                # Define loss function and optimizer \n","                loss_function = nn.MSELoss()\n","                optimizer = torch.optim.Adam(lstm_model.parameters(), lr=ln_rate)\n","\n","                print(f\"\\nTraining for {feature} with sequence length = {seq_length}\\n\")\n","\n","                # Training Loop\n","                for epoch in tqdm(range(num_epochs), total=num_epochs):\n","                    epoch_loss = 0\n","                    for X, y in train_loader:\n","                        \n","                        # Place the training sample to device\n","                        X_dev = X.to(device)\n","                        y_dev = y.to(device)\n","\n","                        # Get the prediction and calculate the loss\n","                        prediction = lstm_model(X_dev)\n","                        loss = loss_function(prediction, y_dev)\n","                        \n","                        # Backpropagation and weights update\n","                        optimizer.zero_grad()\n","                        loss.backward()\n","                        optimizer.step()\n","                        \n","                        # Add the loss\n","                        epoch_loss += loss.item()\n","                    \n","                    num_batches = len(train_loader)\n","                    avg_loss = epoch_loss / num_batches\n","                    # print(f\"Train loss: {avg_loss}\")\n","                \n","                # Save the model\n","                model_path = f\"lstm_{feature}_{epoch+1}.pth\"\n","                # torch.save(lstm_model.state_dict(), model_path)\n","\n","                # Calculate out of sample predictions\n","                outofsample_preds = torch.tensor([]).to(device)\n","                ground = torch.tensor([]).to(device)\n","                lstm_model.eval()\n","                with torch.inference_mode():\n","                    for X, y in test_loader:\n","                        X_dev = X.to(device)\n","                        y_dev = y.to(device)\n","                        prediction = lstm_model(X_dev)\n","                        y_unnorm = y_dev * std + mean\n","                        prediction_unnorm = prediction * std + mean\n","                        outofsample_preds = torch.cat((outofsample_preds, prediction_unnorm), 0)\n","                        ground = torch.cat((ground, y_unnorm), 0)\n","\n","                \n","                # Calculate insample predictions\n","                insample_preds = torch.tensor([]).to(device)\n","                inground = torch.tensor([]).to(device)\n","                lstm_model.eval()\n","                with torch.inference_mode():\n","                    for X, y in test_loader_in_sample:\n","                        X_dev = X.to(device)\n","                        y_dev = y.to(device)\n","                        prediction = lstm_model(X_dev)\n","                        y_unnorm = y_dev * std + mean\n","                        prediction_unnorm = prediction * std + mean\n","                        insample_preds = torch.cat((insample_preds, prediction_unnorm), 0)\n","                        inground = torch.cat((inground, y_unnorm), 0)\n","                \n","                # Write losses in .txt file\n","                # avg_loss_in = loss_function(inground, insample_preds).item()\n","                # msg = f'\\nMSE loss in sample {feature}: {str(avg_loss_in)}\\n'\n","                # # f.write(msg)\n","                # print(f'\\nMSE loss in sample {feature}: {avg_loss_in}')\n","\n","                avg_loss_out = loss_function(ground, outofsample_preds).item()\n","                msg = f'MSE loss out of sample {feature}: {str(avg_loss_out)}\\n'\n","                # f.write(msg)\n","                print(f'MSE loss out of sample {feature}: {avg_loss_out}\\n')\n","                f.write(f\"{i}: {avg_loss_out}\\n\")\n","\n","                seq_losses.append(avg_loss_out)\n","\n","                # Plot insample and out of sample predictions\n","                \n","                # plt.figure(figsize=(15, 9))\n","                # plt.grid(True)\n","                # plt.plot(inground.cpu(), 'r')\n","                # plt.plot(insample_preds.cpu(), 'b', marker='o', linestyle='dashed')\n","                # plt.title(f\"LSTM in-sample predictions for {feature}\")\n","                # plt.legend(['training data', 'predictions'])\n","                # fname= f'lstm_{feature}_insample.png'\n","                # plt.savefig(fname)\n","                # plt.show()\n","\n","                # # to_row = int(len(test_df))\n","                # sample_range = test_df.index\n","                # sample_range = torch.tensor(sample_range[:-seq_length-1]).cpu()\n","\n","                # plt.figure(figsize=(15, 9))\n","                # plt.grid(True)\n","                # plt.plot(sample_range, outofsample_preds.cpu(), 'b', marker='o', linestyle='dashed')\n","                # plt.plot(sample_range, ground.cpu(), 'r')\n","                # plt.title(f\"PREDICTION - {type} - {feature}\")\n","                # plt.xlabel(\"SAMPLES\")\n","                # plt.ylabel(feature)\n","                # plt.legend([f'PREDICTED {feature}', f'ACTUAL {feature}'], loc=\"upper left\")\n","                # fname= f'lstm_{feature}_{type}_predictions.png'\n","                # plt.savefig(fname)\n","                # plt.show()\n","\n","        # plt.figure(figsize=(15, 9))\n","        # plt.grid(True)\n","        # plt.plot(range(1,9), seq_losses)\n","        # plt.title(f\"LSTM out-of-sample loss - sequence length for {feature} and type {type}\")\n","        # plt.xlabel(\"Sequence Length\")\n","        # plt.ylabel(\"Mean Squared Error\")\n","        # plt.legend(['losses'])\n","        # fname = f'lstm_{feature}_{type}_losses_seq_lengths.png'\n","        # plt.savefig(fname)\n","        # plt.show()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PhrTIe5nyXgs","executionInfo":{"status":"ok","timestamp":1674820370045,"user_tz":-120,"elapsed":2662,"user":{"displayName":"Anastasios Tzelepakis","userId":"06794205193158144204"}},"outputId":"94806e4c-e799-4b1f-8b7e-4ed8e2dbe731"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Performing time series analysis for typeA\n","\n","\n","Training for 50um with sequence length = 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 292.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["MSE loss out of sample 50um: 2.6903695389985052e-17\n","\n","\n","Training for 20um with sequence length = 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 289.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["MSE loss out of sample 20um: 3.176448660203246e-17\n","\n","\n","Performing time series analysis for typeB\n","\n","\n","Training for 50um with sequence length = 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 300.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["MSE loss out of sample 50um: 2.0194584815619542e-18\n","\n","\n","Training for 20um with sequence length = 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 288.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["MSE loss out of sample 20um: 2.4916284780485546e-18\n","\n","\n","Performing time series analysis for typeC\n","\n","\n","Training for 50um with sequence length = 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 294.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["MSE loss out of sample 50um: 1.775864958103357e-17\n","\n","\n","Training for 20um with sequence length = 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 265.23it/s]"]},{"output_type":"stream","name":"stdout","text":["MSE loss out of sample 20um: 3.1072877583154424e-17\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Performing time series analysis for typeD\n","\n","\n","Training for 50um with sequence length = 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 277.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["MSE loss out of sample 50um: 2.136128377854649e-18\n","\n","\n","Training for 20um with sequence length = 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 288.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["MSE loss out of sample 20um: 2.163808115897358e-18\n","\n","\n","Performing time series analysis for typeE\n","\n","\n","Training for 50um with sequence length = 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 188.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["MSE loss out of sample 50um: 2.6346068537234986e-18\n","\n","\n","Training for 20um with sequence length = 1\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 50/50 [00:00<00:00, 294.71it/s]"]},{"output_type":"stream","name":"stdout","text":["MSE loss out of sample 20um: 2.540101055149009e-18\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}